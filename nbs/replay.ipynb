{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Uniform\n",
    "\n",
    "from torchtyping import TensorType"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Level Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PrioritizedReplayDistribution:\n",
    "    def __init__(\n",
    "        self,\n",
    "        staleness_coeff: float = 0.1,\n",
    "        temperature: float = 0.1, # the beta coefficient for the P_S distribution\n",
    "    ) -> None:\n",
    "        self.staleness_coeff = staleness_coeff\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def create(\n",
    "        self,\n",
    "        score_levels: TensorType[\"n_visited_levels\"],\n",
    "        last_count_levels: TensorType[\"n_visited_levels\"], # the last episode that each level was played,\n",
    "        last_episode: TensorType[1] # the last episode \n",
    "    ) -> TensorType[\"n_visited_levels\"]:\n",
    "        \"\"\"Create a prioritized level distribution.\"\"\"\n",
    "        \n",
    "        level_scores = torch.pow(\n",
    "            input=F.normalize(score_levels, dim=-1),\n",
    "            exponent=1/self.temperature\n",
    "        )\n",
    "        score_dist = level_scores / level_scores.sum(dim=-1)[:, None]\n",
    "        \n",
    "        stale_scores = last_episode - last_count_levels\n",
    "        stale_dist = stale_scores / stale_scores.sum(dim=-1)\n",
    "        \n",
    "        prioritized_dist = (1 - self.staleness_coeff) * score_dist + self.staleness_coeff * stale_dist\n",
    "        \n",
    "        return prioritized_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PrioritizedReplay:\n",
    "    def __init__(\n",
    "        self,\n",
    "        levels: TensorType[\"n_levels\"],\n",
    "        policy: nn.Module,\n",
    "    ) -> None:\n",
    "        self.levels = levels\n",
    "        # self.visited_levels = torch.zeros_like(levels)\n",
    "        self.visited_count_levels = torch.zeros_like(self.levels)\n",
    "        self.last_count_levels = torch.zeros_like(self.levels)\n",
    "        self.score_levels = torch.zeros_like(levels)\n",
    "        self.last_episode = 0\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.prioritized_dist = PrioritizedReplayDistribution()\n",
    "    \n",
    "    def sample_next_level(\n",
    "        self,\n",
    "        visited_levels: TensorType[\"batch_size\", \"n_visited_levels\"],\n",
    "        score_levels: TensorType[\"batch_size\", \"n_visited_levels\"],\n",
    "        last_count_levels: TensorType[\"batch_size\", \"n_visited_levels\"],\n",
    "        last_episode: TensorType[1]\n",
    "    ) -> TensorType[1]:\n",
    "        \"\"\"Sampling a level from the replay distribution.\"\"\"\n",
    "        # sample replay decision\n",
    "        dist = Bernoulli(probs=0.5)\n",
    "        \n",
    "        # TODO: support batch\n",
    "        mask = ~torch.isin(self.levels, visited_levels)\n",
    "        unseen_levels = self.levels[mask]\n",
    "    \n",
    "        if dist.sample() == 0 and len(unseen_levels) > 0:            \n",
    "            # sample an unseen level\n",
    "            uniform_dist = torch.rand(len(unseen_levels))\n",
    "            selected_index = torch.argmax(uniform_dist)\n",
    "            next_level = unseen_levels[selected_index]\n",
    "            \n",
    "            idx_selected_unseen_level = torch.where(self.levels == next_level)[0][0]\n",
    "            self.visited_count_levels[idx_selected_unseen_level] = 1\n",
    "        else:\n",
    "            # sample a level for replay\n",
    "            prioritized_dist = self.prioritized_dist.create(\n",
    "                score_levels,\n",
    "                last_count_levels,\n",
    "                last_episode\n",
    "            )\n",
    "            \n",
    "            visited_idx = torch.multinomial(prioritized_dist, num_samples=1)\n",
    "            next_level = visited_levels[visited_idx][0] # return the first batch\n",
    "            \n",
    "        return next_level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
