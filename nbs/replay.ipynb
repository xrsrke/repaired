{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Uniform\n",
    "from torchtyping import TensorType\n",
    "\n",
    "from repaired.level import Level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Level Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PrioritizedReplayDistribution:\n",
    "    def __init__(\n",
    "        self,\n",
    "        staleness_coeff: float = 0.1,\n",
    "        temperature: float = 0.1, # the beta coefficient for the P_S distribution\n",
    "    ) -> None:\n",
    "        self.staleness_coeff = staleness_coeff\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def create(\n",
    "        self,\n",
    "        score_levels: Dict[Level, Union[int, float]],\n",
    "        last_episode_levels: Dict[Level, int], # the last episode that each level was played,\n",
    "        last_episode: int # the last episode \n",
    "    ) -> TensorType[\"n_visited_levels\"]:\n",
    "        \"\"\"Create a prioritized level distribution.\"\"\"\n",
    "        \n",
    "        score_levels = torch.tensor([v for v in score_levels.values()])\n",
    "        last_episode_levels = torch.tensor([v for v in last_episode_levels.values()])\n",
    "        \n",
    "        level_scores = torch.pow(\n",
    "            input=F.normalize(score_levels, dim=-1),\n",
    "            exponent=1/self.temperature\n",
    "        )\n",
    "        score_dist = level_scores / level_scores.sum(dim=-1)\n",
    "        \n",
    "        stale_scores = last_episode - last_episode_levels\n",
    "        stale_dist = stale_scores / stale_scores.sum(dim=-1)\n",
    "        \n",
    "        prioritized_dist = (1 - self.staleness_coeff) * score_dist + self.staleness_coeff * stale_dist\n",
    "        \n",
    "        return prioritized_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PrioritizedReplay:\n",
    "    def __init__(\n",
    "        self,\n",
    "        levels: List[Level],\n",
    "    ) -> None:\n",
    "        self.levels = levels\n",
    "        self.visited_levels: List[Level] = []\n",
    "        self.visited_count_levels: Dict[Level, int] = {}\n",
    "        self.last_count_levels: Dict[Level, int] = {}\n",
    "        self.score_levels: Dict[Level, Union[int, float]] = {}\n",
    "        self.last_episode = 0\n",
    "        \n",
    "        self.prioritized_dist = PrioritizedReplayDistribution()\n",
    "    \n",
    "    def sample_next_level(\n",
    "        self,\n",
    "        visited_levels: List[Level],\n",
    "        score_levels: Dict[str, Union[int, float]],\n",
    "        last_episode_levels: Dict[str, int],\n",
    "        last_episode: int\n",
    "    ) -> Level:\n",
    "        \"\"\"Sampling a level from the replay distribution.\"\"\"\n",
    "        # sample replay decision\n",
    "        decision_dist = Bernoulli(probs=0.5)\n",
    "        \n",
    "        # write unseen level by filter level_id\n",
    "        unseen_levels = [level for level in self.levels if level not in visited_levels]\n",
    "\n",
    "        if decision_dist.sample() == 0 and  len(unseen_levels) > 0:            \n",
    "            # sample an unseen level\n",
    "            uniform_dist = torch.rand(len(unseen_levels))\n",
    "            selected_index = torch.argmax(uniform_dist)\n",
    "            next_level = unseen_levels[selected_index]\n",
    "            \n",
    "            self.visited_count_levels[next_level] = 1\n",
    "        else:\n",
    "            # sample a level for replay\n",
    "            prioritized_dist = self.prioritized_dist.create(\n",
    "                score_levels,\n",
    "                last_episode_levels,\n",
    "                last_episode\n",
    "            )\n",
    "            \n",
    "            visited_idx = torch.multinomial(prioritized_dist, num_samples=1)\n",
    "            next_level = visited_levels[visited_idx]\n",
    "            \n",
    "        return next_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
